Directory structure:
└── backend/
    ├── README.md
    ├── docker-compose.yml
    ├── Dockerfile
    ├── requirements.txt
    ├── .env.example
    └── app/
        ├── __init__.py
        ├── main.py
        ├── adapters/
        │   ├── llm/
        │   │   ├── base.py
        │   │   ├── gemini_provider.py
        │   │   └── ollama_provider.py
        │   ├── storage/
        │   │   └── object_storage.py
        │   └── vectorstore/
        │       ├── opensearch_client.py
        │       └── opensearch_vectorstore.py
        ├── api/
        │   ├── __init__.py
        │   └── v1/
        │       ├── __init__.py
        │       └── routes/
        │           ├── __init__.py
        │           ├── auth.py
        │           ├── health.py
        │           ├── learning.py
        │           └── materials.py
        ├── core/
        │   ├── __init__.py
        │   ├── config.py
        │   ├── deps.py
        │   └── logging.py
        ├── db/
        │   ├── __init__.py
        │   ├── base.py
        │   ├── init_db.py
        │   ├── session.py
        │   └── models/
        │       ├── __init__.py
        │       ├── learning_material.py
        │       └── user.py
        ├── services/
        │   ├── __init__.py
        │   ├── auth_service.py
        │   ├── materials_service.py
        │   ├── prereq_service.py
        │   ├── question_service.py
        │   ├── rag_service.py
        │   └── session_service.py
        ├── services_impl/
        │   ├── __init__.py
        │   ├── auth_service_impl.py
        │   ├── materials_service_impl.py
        │   ├── prereq_llm_impl.py
        │   ├── question_llm_impl.py
        │   ├── rag_service_opensearch_impl.py
        │   └── session_service_impl.py
        └── workers/
            ├── __init__.py
            └── ingestion_worker.py

================================================
FILE: README.md
================================================
# open_learning_assistant – Backend

Backend for **open_learning_assistant** – an open source learning assistant that helps students learn complex topics from their own PDFs/TXT files using Language Models, prerequisite graphs, and gamified questions.

This repo contains:

- A **FastAPI** backend (`app/main.py`)
- A **service / service_impl** architecture for business logic
- Integration hooks for:
  - **OpenSearch** as a vector store
  - **Ollama** (local) or **Gemini** as LLM providers
  - **Postgres** as the relational database
  - **Local filesystem storage** for uploaded materials

> ⚠️ Status: Early skeleton. Some pieces are intentionally stubbed (RAG search, ingestion embeddings, real auth).

---

## Architecture (Backend)

Key concepts:

- `api/v1/routes/*` – HTTP routes (FastAPI routers)
- `services/*` – abstract service interfaces
- `services_impl/*` – concrete implementations (service-serviceimpl pattern)
- `adapters/*` – integration adapters (LLM, vector store, storage)
- `db/*` – SQLAlchemy models + session
- `workers/ingestion_worker.py` – ingestion pipeline skeleton

Rough data flow for MVP:

1. Student uploads a PDF → `/api/v1/materials/upload`
2. Backend stores file via `ObjectStorage` and creates a `learning_materials` row
3. Ingestion worker (later) will:
   - Parse PDF → text
   - Chunk text
   - Embed chunks
   - Index into OpenSearch
4. Student asks questions → `/api/v1/learning/ask`
   - RAG service retrieves relevant chunks (OpenSearch)
   - LLM generates an answer + follow-up questions

---

## Tech Stack

- **Language / Framework**
  - Python 3.11
  - FastAPI
  - Uvicorn

- **Database**
  - Postgres
  - SQLAlchemy ORM

- **Vector Store**
  - OpenSearch with `knn_vector` fields

- **LLMs**
  - Ollama (default)
  - Gemini (via `google-genai`)

- **Other**
  - PyMuPDF (`fitz`) for PDF parsing
  - Docker + docker-compose

---

## Getting Started

### 1. Clone & create your `.env`

```bash
cd backend
cp .env.example .env



================================================
FILE: docker-compose.yml
================================================
services:
  backend:
    build: .
    env_file:
      - .env
    depends_on:
      - postgres
      - opensearch
    ports:
      - "8000:8000"
    volumes:
      - ./data:/data

  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: ola
      POSTGRES_USER: user
      POSTGRES_PASSWORD: pass
    ports:
      - "5433:5433"

  opensearch:
    image: opensearchproject/opensearch:2.15.0
    environment:
      - discovery.type=single-node
      - OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m
      - DISABLE_SECURITY_PLUGIN=true
    ports:
      - "9200:9200"




================================================
FILE: Dockerfile
================================================
# backend/Dockerfile
FROM python:3.11-slim

ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1

WORKDIR /app

# System deps if you need them (you can trim later)
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    curl \
    && rm -rf /var/lib/apt/lists/*

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY app ./app

EXPOSE 8000

CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]



================================================
FILE: requirements.txt
================================================
fastapi
uvicorn[standard]

# DB
SQLAlchemy>=2.0
psycopg2-binary

# Vector store
opensearch-py

# File uploads
python-multipart

# LLM & content
pymupdf
ollama
google-genai

# Config & env
python-dotenv
pydantic
pydantic-settings

# Dev
ipykernel  # dev-dependency



================================================
FILE: .env.example
================================================
# FastAPI
PROJECT_NAME="open_learning_assistant"

# DB
DATABASE_URL="postgresql+psycopg2://user:pass@postgres:5432/ola"

# OpenSearch
OPENSEARCH_HOST="http://opensearch:9200"
OPENSEARCH_USER="admin"
OPENSEARCH_PASSWORD=""
OPENSEARCH_INDEX="content_chunks"

# LLM
LLM_PROVIDER="ollama"  # or "gemini"

# Ollama
OLLAMA_BASE_URL="http://ollama:11434"
OLLAMA_MODEL="qwen3-vl:2b"

# Gemini
GEMINI_API_KEY=""
GEMINI_MODEL="gemini-2.0-flash"

# Storage
STORAGE_BACKEND="local"
STORAGE_BASE_PATH="/data/materials"



================================================
FILE: app/__init__.py
================================================



================================================
FILE: app/main.py
================================================
# app/main.py
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

from app.core.config import Settings
from app.api.v1.routes import auth, materials, learning, health
from app.core.logging import configure_logging

def create_app() -> FastAPI:
    configure_logging()
    app = FastAPI(
        title="open_learning_assistant",
        version="0.1.0",
    )

    # CORS (adjust for your FE origin)
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    # Routers
    app.include_router(health.router, prefix="/api/v1/health", tags=["health"])
    app.include_router(auth.router, prefix="/api/v1/auth", tags=["auth"])
    app.include_router(materials.router, prefix="/api/v1/materials", tags=["materials"])
    app.include_router(learning.router, prefix="/api/v1/learning", tags=["learning"])

    return app


app = create_app()



================================================
FILE: app/adapters/llm/base.py
================================================
# app/adapters/llm/base.py
from abc import ABC, abstractmethod
from typing import Tuple, List

class LLMClient(ABC):
    @abstractmethod
    async def chat(self, prompt: str) -> str:
        raise NotImplementedError

    @abstractmethod
    async def chat_with_followups(self, prompt: str) -> Tuple[str, List[str]]:
        """Return (answer, followup_questions)."""
        raise NotImplementedError



================================================
FILE: app/adapters/llm/gemini_provider.py
================================================
# app/adapters/llm/gemini_provider.py
from typing import Tuple, List
from google import genai
from app.adapters.llm.base import LLMClient

class GeminiClient(LLMClient):
    def __init__(self, api_key: str, model: str) -> None:
        self.client = genai.Client(api_key=api_key)
        self.model = model

    async def chat(self, prompt: str) -> str:
        # google-genai is sync; wrap in thread executor if you want real async
        resp = self.client.models.generate_content(
            model=self.model,
            contents=prompt,
        )
        return resp.text

    async def chat_with_followups(self, prompt: str) -> Tuple[str, List[str]]:
        full_prompt = f"""
{prompt}

After answering, suggest 3 short followup questions.
Return JSON with keys: answer, followups.
"""
        resp = self.client.models.generate_content(
            model=self.model,
            contents=full_prompt,
        )
        import json
        parsed = json.loads(resp.text)
        return parsed["answer"], parsed.get("followups", [])



================================================
FILE: app/adapters/llm/ollama_provider.py
================================================
# app/adapters/llm/ollama_provider.py
import httpx
from typing import Tuple, List
from app.adapters.llm.base import LLMClient

class OllamaClient(LLMClient):
    def __init__(self, base_url: str, model: str) -> None:
        self.base_url = base_url.rstrip("/")
        self.model = model

    async def chat(self, prompt: str) -> str:
        async with httpx.AsyncClient() as client:
            resp = await client.post(
                f"{self.base_url}/api/chat",
                json={
                    "model": self.model,
                    "messages": [{"role": "user", "content": prompt}],
                },
                timeout=60,
            )
            resp.raise_for_status()
            data = resp.json()
            return data["message"]["content"]

    async def chat_with_followups(self, prompt: str) -> Tuple[str, List[str]]:
        combined_prompt = f"""
{prompt}

After answering, suggest 3 short followup questions for the student.
Return JSON with:
- "answer": string
- "followups": list of strings
"""
        async with httpx.AsyncClient() as client:
            resp = await client.post(
                f"{self.base_url}/api/chat",
                json={
                    "model": self.model,
                    "messages": [{"role": "user", "content": combined_prompt}],
                },
                timeout=60,
            )
            resp.raise_for_status()
            data = resp.json()
            content = data["message"]["content"]

        # you can tighten this later with a json schema
        # for now assume the model returns valid JSON
        import json
        parsed = json.loads(content)
        return parsed["answer"], parsed.get("followups", [])



================================================
FILE: app/adapters/storage/object_storage.py
================================================
# app/adapters/storage/object_storage.py
import os
from uuid import uuid4
from abc import ABC, abstractmethod
from fastapi import UploadFile

from app.core.config import settings


class StorageBackend(ABC):
    """
    Abstract storage backend.
    """

    @abstractmethod
    async def save(self, file: UploadFile) -> str:
        """
        Persist the uploaded file and return a path / URI that can be used later.
        """
        raise NotImplementedError


class LocalFileStorage(StorageBackend):
    """
    Local filesystem storage.

    Writes files under STORAGE_BASE_PATH (e.g. /data/materials inside the container),
    which is mounted as a volume in docker-compose.
    """

    def __init__(self, base_path: str | None = None) -> None:
        self.base_path = base_path or settings.STORAGE_BASE_PATH
        os.makedirs(self.base_path, exist_ok=True)

    async def save(self, file: UploadFile) -> str:
        contents = await file.read()
        ext = os.path.splitext(file.filename or "")[1]
        filename = f"{uuid4().hex}{ext}"
        path = os.path.join(self.base_path, filename)

        with open(path, "wb") as f:
            f.write(contents)

        # Return absolute path inside the container. For now we just store this in DB.
        return path



================================================
FILE: app/adapters/vectorstore/opensearch_client.py
================================================
# app/adapters/vectorstore/opensearch_client.py
from opensearchpy import OpenSearch
from app.core.config import settings

def get_opensearch_client() -> OpenSearch:
    return OpenSearch(
        hosts=[settings.OPENSEARCH_HOST],
        http_auth=(settings.OPENSEARCH_USER, settings.OPENSEARCH_PASSWORD),
        use_ssl=False,  # adjust if you enable SSL
        verify_certs=False,
    )



================================================
FILE: app/adapters/vectorstore/opensearch_vectorstore.py
================================================
# app/adapters/vectorstore/opensearch_vectorstore.py
from typing import List, Dict, Any
from opensearchpy import OpenSearch

class OpenSearchVectorStore:
    def __init__(self, client: OpenSearch, index_name: str) -> None:
        self.client = client
        self.index_name = index_name
        self._ensure_index()

    def _ensure_index(self) -> None:
        if self.client.indices.exists(self.index_name):
            return

        body = {
            "settings": {
                "index": {
                    "knn": True,
                    "knn.algo_param.ef_search": 100,
                }
            },
            "mappings": {
                "properties": {
                    "material_id": {"type": "integer"},
                    "topic_id": {"type": "integer"},
                    "chunk_id": {"type": "keyword"},
                    "content": {"type": "text"},
                    "page": {"type": "integer"},
                    "embedding": {
                        "type": "knn_vector",
                        "dimension": 1536,  # set to your embedding size
                    },
                }
            },
        }
        self.client.indices.create(index=self.index_name, body=body)

    def index_chunk(
        self,
        material_id: int,
        topic_id: int | None,
        chunk_id: str,
        content: str,
        embedding: list[float],
        page: int | None = None,
    ) -> None:
        doc = {
            "material_id": material_id,
            "topic_id": topic_id,
            "chunk_id": chunk_id,
            "content": content,
            "page": page,
            "embedding": embedding,
        }
        self.client.index(index=self.index_name, id=chunk_id, body=doc)

    def search(
        self,
        query: str,
        material_id: int,
        topic_id: int | None,
        k: int = 5,
    ) -> List[Dict[str, Any]]:
        # in a real system you should embed `query` via embedding model
        # here we assume you already have the query embedding
        raise NotImplementedError("Implement search with query embeddings")



================================================
FILE: app/api/__init__.py
================================================



================================================
FILE: app/api/v1/__init__.py
================================================



================================================
FILE: app/api/v1/routes/__init__.py
================================================



================================================
FILE: app/api/v1/routes/auth.py
================================================
# app/api/v1/routes/auth.py
from fastapi import APIRouter

router = APIRouter()

@router.post("/signup")
async def signup_stub():
    # TODO: implement real signup using AuthService
    return {"message": "signup not implemented yet"}

@router.post("/login")
async def login_stub():
    # TODO: implement real login using AuthService + JWT
    return {"access_token": "dummy-token", "token_type": "bearer"}



================================================
FILE: app/api/v1/routes/health.py
================================================
# app/api/v1/routes/health.py
from fastapi import APIRouter

router = APIRouter()

@router.get("/ping")
async def ping():
    return {"status": "ok"}



================================================
FILE: app/api/v1/routes/learning.py
================================================
# app/api/v1/routes/learning.py
from fastapi import APIRouter, Depends
from pydantic import BaseModel
from typing import Optional

from app.core.deps import get_rag_service
from app.services.rag_service import RAGService

router = APIRouter()

# TODO: replace with real auth
def get_current_user_id() -> int:
    return 1


class AskQuestionRequest(BaseModel):
    material_id: int
    topic_id: Optional[int] = None
    question: str


@router.post("/ask")
async def ask_question(
    payload: AskQuestionRequest,
    user_id: int = Depends(get_current_user_id),
    rag_service: RAGService = Depends(get_rag_service),
):
    result = await rag_service.answer_question(
        user_id=user_id,
        material_id=payload.material_id,
        topic_id=payload.topic_id,
        question=payload.question,
    )
    return result



================================================
FILE: app/api/v1/routes/materials.py
================================================
# app/api/v1/routes/materials.py
from fastapi import APIRouter, Depends, UploadFile, File, status
from typing import List

from app.core.deps import get_materials_service
from app.services.materials_service import MaterialsService

router = APIRouter()

# TODO: replace dummy user_id with real auth
def get_current_user_id() -> int:
    return 1

@router.post("/upload", status_code=status.HTTP_201_CREATED)
async def upload_material(
    file: UploadFile = File(...),
    user_id: int = Depends(get_current_user_id),
    service: MaterialsService = Depends(get_materials_service),
):
    material_id = await service.upload_material(user_id=user_id, file=file)
    return {"material_id": material_id}


@router.get("/", response_model=List[dict])
async def list_materials(
    user_id: int = Depends(get_current_user_id),
    service: MaterialsService = Depends(get_materials_service),
):
    return await service.list_materials(user_id)



================================================
FILE: app/core/__init__.py
================================================



================================================
FILE: app/core/config.py
================================================
# app/core/config.py
import os
from functools import lru_cache

from dotenv import load_dotenv
from pydantic import BaseModel, AnyHttpUrl

load_dotenv()


class Settings(BaseModel):
    # FastAPI
    PROJECT_NAME: str = os.getenv("PROJECT_NAME", "open_learning_assistant")
    API_V1_PREFIX: str = os.getenv("API_V1_PREFIX", "/api/v1")

    # DB
    DATABASE_URL: str = os.getenv(
        "DATABASE_URL",
        "postgresql+psycopg2://user:pass@postgres:5433/ola",
    )

    # OpenSearch
    OPENSEARCH_HOST: str = os.getenv("OPENSEARCH_HOST", "http://opensearch:9200")
    OPENSEARCH_USER: str = os.getenv("OPENSEARCH_USER", "admin")
    OPENSEARCH_PASSWORD: str = os.getenv("OPENSEARCH_PASSWORD", "admin")
    OPENSEARCH_INDEX: str = os.getenv("OPENSEARCH_INDEX", "content_chunks")

    # LLM
    LLM_PROVIDER: str = os.getenv("LLM_PROVIDER", "ollama")  # "ollama" | "gemini"
    OLLAMA_BASE_URL: AnyHttpUrl | None = os.getenv(
        "OLLAMA_BASE_URL",
        "http://ollama:11434",
    )
    OLLAMA_MODEL: str = os.getenv("OLLAMA_MODEL", "llama3")
    GEMINI_API_KEY: str | None = os.getenv("GEMINI_API_KEY")
    GEMINI_MODEL: str = os.getenv("GEMINI_MODEL", "gemini-2.0-flash")

    # Object storage
    STORAGE_BACKEND: str = os.getenv("STORAGE_BACKEND", "local")  # local | s3 | minio
    STORAGE_BASE_PATH: str = os.getenv("STORAGE_BASE_PATH", "/data/materials")


@lru_cache
def get_settings() -> Settings:
    return Settings()


settings = get_settings()



================================================
FILE: app/core/deps.py
================================================
# app/core/deps.py
from fastapi import Depends
from sqlalchemy.orm import Session

from app.core.config import settings
from app.db.session import get_db

from app.adapters.llm.base import LLMClient
from app.adapters.llm.ollama_provider import OllamaClient
from app.adapters.llm.gemini_provider import GeminiClient

from app.adapters.vectorstore.opensearch_client import get_opensearch_client
from app.adapters.vectorstore.opensearch_vectorstore import OpenSearchVectorStore

from app.adapters.storage.object_storage import StorageBackend, LocalFileStorage

from app.services.rag_service import RAGService
from app.services_impl.rag_service_opensearch_impl import RAGServiceOpenSearchImpl

from app.services.materials_service import MaterialsService
from app.services_impl.materials_service_impl import MaterialsServiceImpl


def get_llm_client() -> LLMClient:
    if settings.LLM_PROVIDER == "gemini":
        return GeminiClient(
            api_key=settings.GEMINI_API_KEY,
            model=settings.GEMINI_MODEL,
        )
    else:
        # default ollama
        return OllamaClient(
            base_url=str(settings.OLLAMA_BASE_URL),
            model=settings.OLLAMA_MODEL,
        )


def get_vector_store() -> OpenSearchVectorStore:
    client = get_opensearch_client()
    return OpenSearchVectorStore(
        client=client,
        index_name=settings.OPENSEARCH_INDEX,
    )


def get_storage_backend() -> StorageBackend:
    # later you can branch on STORAGE_BACKEND == "s3" | "minio"
    return LocalFileStorage()


def get_rag_service(
    db: Session = Depends(get_db),
    vector_store: OpenSearchVectorStore = Depends(get_vector_store),
    llm: LLMClient = Depends(get_llm_client),
) -> RAGService:
    return RAGServiceOpenSearchImpl(
        db=db,
        vector_store=vector_store,
        llm=llm,
    )


def get_materials_service(
    db: Session = Depends(get_db),
    vector_store: OpenSearchVectorStore = Depends(get_vector_store),
    storage: StorageBackend = Depends(get_storage_backend),
) -> MaterialsService:
    return MaterialsServiceImpl(
        db=db,
        vector_store=vector_store,
        storage=storage,
    )



================================================
FILE: app/core/logging.py
================================================
# app/core/logging.py
import logging
import sys

LOG_FORMAT = "[%(asctime)s] [%(levelname)s] %(name)s - %(message)s"

def configure_logging(level: int = logging.INFO) -> None:
    logging.basicConfig(
        level=level,
        format=LOG_FORMAT,
        handlers=[
            logging.StreamHandler(sys.stdout),
        ],
    )

def get_logger(name: str) -> logging.Logger:
    return logging.getLogger(name)



================================================
FILE: app/db/__init__.py
================================================
# app/db/__init__.py
"""
Database package: base, session, models, init_db utilities.
"""



================================================
FILE: app/db/base.py
================================================
# app/db/base.py
from sqlalchemy.orm import declarative_base

Base = declarative_base()
from app.db import models 



================================================
FILE: app/db/init_db.py
================================================
# app/db/init_db.py
from app.db.base import Base
from app.db.session import engine

def init_db() -> None:
    """
    Simple metadata.create_all for early iterations.

    Later you can replace this with Alembic migrations.
    """
    Base.metadata.create_all(bind=engine)


if __name__ == "__main__":
    init_db()



================================================
FILE: app/db/session.py
================================================
# app/db/session.py
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

from app.core.config import settings

engine = create_engine(
    settings.DATABASE_URL,
    future=True,
)

SessionLocal = sessionmaker(
    autocommit=False,
    autoflush=False,
    bind=engine,
)


def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()



================================================
FILE: app/db/models/__init__.py
================================================
# app/db/models/__init__.py
from app.db.models.user import User  
from app.db.models.learning_material import LearningMaterial  



================================================
FILE: app/db/models/learning_material.py
================================================
# app/db/models/learning_material.py
from datetime import datetime
from sqlalchemy import Column, Integer, String, DateTime, ForeignKey
from sqlalchemy.orm import relationship

from app.db.base import Base

class LearningMaterial(Base):
    __tablename__ = "learning_materials"

    id = Column(Integer, primary_key=True, index=True)
    owner_id = Column(Integer, ForeignKey("users.id"), nullable=False)

    filename = Column(String(512), nullable=False)
    path = Column(String(1024), nullable=False)
    status = Column(String(50), default="PENDING", nullable=False)

    created_at = Column(DateTime, default=datetime.utcnow, nullable=False)
    updated_at = Column(
        DateTime, default=datetime.utcnow, onupdate=datetime.utcnow, nullable=False
    )

    owner = relationship("User", backref="materials")



================================================
FILE: app/db/models/user.py
================================================
# app/db/models/user.py
from datetime import datetime
from sqlalchemy import Column, Integer, String, DateTime
from app.db.base import Base

class User(Base):
    __tablename__ = "users"

    id = Column(Integer, primary_key=True, index=True)
    email = Column(String(255), unique=True, index=True, nullable=False)
    password = Column(String(255), nullable=False)
    name = Column(String(255), nullable=True)
    created_at = Column(DateTime, default=datetime.utcnow, nullable=False)



================================================
FILE: app/services/__init__.py
================================================



================================================
FILE: app/services/auth_service.py
================================================



================================================
FILE: app/services/materials_service.py
================================================
# app/services/materials_service.py
from abc import ABC, abstractmethod
from typing import List
from fastapi import UploadFile

class MaterialsService(ABC):
    @abstractmethod
    async def upload_material(
        self,
        user_id: int,
        file: UploadFile,
    ) -> int:
        """Store file, create learning_material row, enqueue ingestion job. Returns material_id."""
        raise NotImplementedError

    @abstractmethod
    async def list_materials(self, user_id: int) -> List[dict]:
        raise NotImplementedError



================================================
FILE: app/services/prereq_service.py
================================================



================================================
FILE: app/services/question_service.py
================================================



================================================
FILE: app/services/rag_service.py
================================================
# app/services/rag_service.py
from abc import ABC, abstractmethod
from typing import List, Dict, Any

class RAGService(ABC):
    @abstractmethod
    async def answer_question(
        self,
        user_id: int,
        material_id: int,
        topic_id: int | None,
        question: str,
    ) -> Dict[str, Any]:
        """Return answer + sources + followups."""
        raise NotImplementedError



================================================
FILE: app/services/session_service.py
================================================



================================================
FILE: app/services_impl/__init__.py
================================================



================================================
FILE: app/services_impl/auth_service_impl.py
================================================



================================================
FILE: app/services_impl/materials_service_impl.py
================================================
# app/services_impl/materials_service_impl.py
from typing import List
from fastapi import UploadFile
from sqlalchemy.orm import Session

from app.services.materials_service import MaterialsService
from app.adapters.storage.object_storage import StorageBackend
from app.db import models


class MaterialsServiceImpl(MaterialsService):
    def __init__(
        self,
        db: Session,
        vector_store,
        storage: StorageBackend,
    ) -> None:
        self.db = db
        self.storage = storage
        self.vector_store = vector_store

    async def upload_material(
        self,
        user_id: int,
        file: UploadFile,
    ) -> int:
        # 1. store raw file via storage backend
        path = await self.storage.save(file)

        # 2. create DB row
        material = models.learning_material.LearningMaterial(
            owner_id=user_id,
            filename=file.filename,
            path=path,
            status="PENDING",
        )
        self.db.add(material)
        self.db.commit()
        self.db.refresh(material)

        # 3. enqueue ingestion job (Celery/RQ) – left as TODO
        # enqueue_ingestion_job(material.id)

        return material.id

    async def list_materials(self, user_id: int) -> List[dict]:
        rows = (
            self.db.query(models.learning_material.LearningMaterial)
            .filter_by(owner_id=user_id)
            .all()
        )
        return [
            {
                "id": m.id,
                "filename": m.filename,
                "status": m.status,
            }
            for m in rows
        ]



================================================
FILE: app/services_impl/prereq_llm_impl.py
================================================



================================================
FILE: app/services_impl/question_llm_impl.py
================================================



================================================
FILE: app/services_impl/rag_service_opensearch_impl.py
================================================
# app/services_impl/rag_service_opensearch_impl.py
from typing import Dict, Any
from sqlalchemy.orm import Session

from app.services.rag_service import RAGService
from app.adapters.llm.base import LLMClient
from app.adapters.vectorstore.opensearch_vectorstore import OpenSearchVectorStore

class RAGServiceOpenSearchImpl(RAGService):
    def __init__(
        self,
        db: Session,
        vector_store: OpenSearchVectorStore,
        llm: LLMClient,
    ) -> None:
        self.db = db
        self.vector_store = vector_store
        self.llm = llm

    async def answer_question(
        self,
        user_id: int,
        material_id: int,
        topic_id: int | None,
        question: str,
    ) -> Dict[str, Any]:
        # 1. retrieval
        docs = self.vector_store.search(
            query=question,
            material_id=material_id,
            topic_id=topic_id,
            k=5,
        )

        context_blocks = [d["content"] for d in docs]
        sources = [
            {"chunk_id": d["chunk_id"], "page": d.get("page")}
            for d in docs
        ]

        # 2. prompt LLM
        prompt = self._build_prompt(question, context_blocks)
        answer, followups = await self.llm.chat_with_followups(prompt)

        return {
            "answer": answer,
            "sources": sources,
            "followups": followups,
        }

    def _build_prompt(self, question: str, context_blocks: list[str]) -> str:
        context = "\n\n---\n\n".join(context_blocks)
        return f"""
You are a helpful tutoring assistant.

Use ONLY the context below to answer the question.
If something is unclear or missing, say so explicitly.

Context:
{context}

Question: {question}

Return a clear explanation suitable for a student.
"""



================================================
FILE: app/services_impl/session_service_impl.py
================================================



================================================
FILE: app/workers/__init__.py
================================================



================================================
FILE: app/workers/ingestion_worker.py
================================================
# app/workers/ingestion_worker.py
"""
Skeleton ingestion worker.

Later you can turn this into a Celery/RQ worker or a simple cron job
that scans for PENDING learning_materials and processes them.
"""

import fitz  # PyMuPDF
from sqlalchemy.orm import Session

from app.db.session import SessionLocal
from app.db.models.learning_material import LearningMaterial


def extract_text_from_pdf(path: str) -> str:
    doc = fitz.open(path)
    texts: list[str] = []
    for page in doc:
        texts.append(page.get_text())
    return "\n".join(texts)


def simple_chunk(text: str, max_chars: int = 1500) -> list[str]:
    chunks: list[str] = []
    start = 0
    while start < len(text):
        end = min(start + max_chars, len(text))
        chunks.append(text[start:end])
        start = end
    return chunks


def process_material(material_id: int) -> None:
    db: Session = SessionLocal()
    try:
        material = db.get(LearningMaterial, material_id)
        if not material:
            print(f"Material {material_id} not found")
            return

        print(f"Processing material {material_id}: {material.path}")
        text = extract_text_from_pdf(material.path)
        chunks = simple_chunk(text)

        # TODO: for each chunk:
        #  - generate embedding with your chosen model
        #  - store chunk metadata in DB (content_chunks table)
        #  - index into OpenSearchVectorStore

        material.status = "READY"
        db.add(material)
        db.commit()
        print(f"Material {material_id} marked as READY")
    finally:
        db.close()


if __name__ == "__main__":
    # quick manual test:
    # process_material(1)
    pass


